{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Filtering\n",
    "\n",
    "This notebook is used to apply additional filtering criteria to our ECHR-QA Dataset.\n",
    "Each ECHR-QA pair must:\n",
    "- Have at least one citation\n",
    "- All passages of the citation must be retrievable (and will be saved with the citation)\n",
    "- Questions must not mention a case\n",
    "\n",
    "Final Result:\n",
    "- Each ECHR-QA pair entry consist of (question, answer, citations) where the citations are all cited cases in the answer.\n",
    "\n",
    "General strategy:\n",
    "- We parse the case law guides on a sentence level and save the citations for each sentence (guide_id, paragraph, sentence, citations)\n",
    "- We attempt to retrieve the citations for each sentence so we know which sentences are usable\n",
    "- We attempt to create a QA pair based on the entire case law guides \n",
    "- We filter out generated QA pairs that do not meet all of the criteria mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences_df = pd.read_csv('data/sentences_with_citations_usable.csv')\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    case_name: str\n",
    "    case_id: str\n",
    "    paragraph_numbers: list[int]\n",
    "    paragraphs_map: dict[int, str] | None = {}\n",
    "\n",
    "def clean_case_id(case_id: str):\n",
    "    if \"(F)\" in case_id:\n",
    "        raise ValueError(f\"Case id contains (F): {case_id}\")\n",
    "    if \"unknown\" in case_id.lower():\n",
    "        raise ValueError(f\"Case id unknown: {case_id}\")\n",
    "    if \"(\" in case_id:\n",
    "        case_id = case_id.split(\"(\")[0]\n",
    "    return case_id\n",
    "\n",
    "def merge_citations(list_of_citations: list[Citation]):\n",
    "    merged = {clean_case_id(c.case_id): Citation(case_name=c.case_name, case_id=clean_case_id(c.case_id), paragraph_numbers=[]) for c in list_of_citations}\n",
    "    for c in list_of_citations:\n",
    "        merged[clean_case_id(c.case_id)].paragraph_numbers.extend(c.paragraph_numbers)\n",
    "        merged[clean_case_id(c.case_id)].paragraph_numbers = list(set(merged[clean_case_id(c.case_id)].paragraph_numbers))\n",
    "        merged[clean_case_id(c.case_id)].paragraph_numbers.sort()\n",
    "        \n",
    "    return list(merged.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test merge citations\n",
    "import json\n",
    "\n",
    "\n",
    "list_of_citations_1 = [{\"case_name\": \"ila\\u015fcu and others v. moldova and russia\", \"case_id\": \"001-61886\", \"paragraph_numbers\": [348, 349, 350, 351, 352]}, {\"case_name\": \"ivan\\u0163oc and others v. moldova and russia\", \"case_id\": \"001-107480\", \"paragraph_numbers\": [111]}, {\"case_name\": \"Catan and Others v. the Republic of Moldova and Russia\", \"case_id\": \"001-114082\", \"paragraph_numbers\": [148]}]\n",
    "list_of_citations_1 = [Citation(**c) for c in list_of_citations_1]\n",
    "list_of_citations_2 = [{\"case_name\": \"mamasakhlisi and others v. georgia and russia\", \"case_id\": \"001-223361\", \"paragraph_numbers\": [398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410]}]\n",
    "list_of_citations_2 = [Citation(**c) for c in list_of_citations_2]\n",
    "list_of_citations = list_of_citations_1 + list_of_citations_2\n",
    "merged = merge_citations(list_of_citations)\n",
    "print(json.dumps([m.model_dump() for m in merged], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "\n",
    "def get_sentences_spacy(text: str):\n",
    "    doc = nlp(text)\n",
    "    return [sentence.text for sentence in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echr_qa_dataset_df = pd.read_csv('data/echr_qa_dataset_v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_paragraph_number(text):\n",
    "  pattern = r\"^(\\d+)\\.\"\n",
    "  match = re.match(pattern, text)\n",
    "  return int(match.group(1)) if match else None\n",
    "\n",
    "def get_paragraphs(data: list[dict]):\n",
    "    paragraphs = {}\n",
    "    def get_paragraphs_rec(data):\n",
    "        paragraph_number = get_paragraph_number(data[\"content\"])\n",
    "        if paragraph_number and paragraph_number not in paragraphs:\n",
    "            paragraph = data[\"content\"]\n",
    "            paragraphs[paragraph_number] = paragraph\n",
    "        for e in data[\"elements\"]:\n",
    "            res = get_paragraphs_rec(e)\n",
    "            if res:\n",
    "                return res\n",
    "        return None\n",
    "    for d in data:\n",
    "        get_paragraphs_rec(d)\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "db_path = \"data/echr_2_0_0.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "c = conn.cursor()\n",
    "\n",
    "cases = c.execute(\"SELECT * FROM 'case'\")\n",
    "cases = cases.fetchall()\n",
    "\n",
    "cases_map = {} # maps case_id to paragraph_number to paragraph_text\n",
    "\n",
    "for case in cases:\n",
    "    case_id = case[0]\n",
    "    data = case[-1]\n",
    "    data = json.loads(data)\n",
    "    paragraphs = get_paragraphs(data)\n",
    "    # print(json.dumps(paragraphs, indent=4))\n",
    "    cases_map[case_id] = paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def available_paragraphs(text: str):\n",
    "    i = 1\n",
    "    while f\"\\n{i}\" in text:\n",
    "        i += 1\n",
    "    return i - 1\n",
    "\n",
    "def get_paragraphs_for_case_id(case_id: str):\n",
    "    url = f\"https://hudoc.echr.coe.int/app/conversion/docx/html/body?library=ECHR&id={case_id}\"\n",
    "    res = requests.get(url)\n",
    "    data = res.text\n",
    "\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    n = available_paragraphs(text)\n",
    "\n",
    "    paragraphs = {}\n",
    "    for i in range(1, n):\n",
    "        _, _, after = text.partition(f\"\\n{i}\")\n",
    "        paragraph, _, text = after.partition(f\"\\n{i+1}\")\n",
    "        text = f\"\\n{i+1}\" + text\n",
    "        paragraphs[i] = re.sub(r'\\s+', ' ', paragraph).strip()\n",
    "    paragraphs[n] = text[0:600]\n",
    "    return paragraphs\n",
    "\n",
    "print(json.dumps(get_paragraphs_for_case_id(\"001-98238\"), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def retrieve_citations(citations: list[Citation]):\n",
    "    citations = copy.deepcopy(citations)\n",
    "    error = False\n",
    "    \n",
    "    for citation in citations:\n",
    "        case_id = citation.case_id\n",
    "\n",
    "        if \"(F)\" in case_id or \"unknown\" in case_id.lower() or \"UNIDENTIFIABLE\" in case_id:\n",
    "            error = True\n",
    "            continue\n",
    "\n",
    "        case_id = clean_case_id(case_id)\n",
    "\n",
    "        retrieved_case = False\n",
    "        if case_id not in cases_map:\n",
    "            # we attempt to retrieve the paragraphs from the web and update our map\n",
    "            retrieved_case = True\n",
    "            paragraphs = get_paragraphs_for_case_id(case_id)\n",
    "            if not paragraphs:\n",
    "                print(f\"Failed retrieving paragraphs: https://hudoc.echr.coe.int/app/conversion/docx/html/body?library=ECHR&id={case_id}\")\n",
    "                error = True\n",
    "            \n",
    "            # we save it anyway as we don't want to keep trying to retrieve it\n",
    "            cases_map[case_id] = paragraphs\n",
    "        else: \n",
    "            paragraphs = cases_map[case_id]\n",
    "        \n",
    "        paragraphs_map = {}\n",
    "        for paragraph_number in citation.paragraph_numbers:\n",
    "            if paragraph_number not in paragraphs:\n",
    "                print(f'Paragraph {paragraph_number} not found for{\" (retrieved)\" if retrieved_case else \"\"}: https://hudoc.echr.coe.int/eng?i={case_id}')\n",
    "                error = True\n",
    "                break\n",
    "            paragraphs_map[paragraph_number] = paragraphs[paragraph_number]\n",
    "\n",
    "        citation.paragraphs_map = paragraphs_map\n",
    "    return citations, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! only run if needed !\n",
    "\n",
    "usable = []\n",
    "\n",
    "for i, row in sentences_df.iterrows():\n",
    "    citations = json.loads(row[\"citations\"])\n",
    "    citations = [Citation(**c) for c in citations]\n",
    "    citations, error = retrieve_citations(citations)\n",
    "    if error:\n",
    "        usable.append(False)\n",
    "    else:\n",
    "        usable.append(True)\n",
    "\n",
    "sentences_df[\"usable\"] = usable\n",
    "\n",
    "sentences_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the true and false values in the usable column\n",
    "print(sentences_df[\"usable\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citations(sentences: list[str]):\n",
    "    # only returns citations if they are valid, else none\n",
    "    citations = []\n",
    "    for sentence in sentences:\n",
    "        find_sentence_df = sentences_df[sentences_df['sentence'] == sentence]\n",
    "\n",
    "        if len(find_sentence_df) == 0:\n",
    "            print(\"Sentence not found in guide, removing pair...\")\n",
    "            return None\n",
    "            \n",
    "        sentence_citations = json.loads(find_sentence_df.iloc[0][\"citations\"])\n",
    "        sentence_citations = [Citation(**c) for c in sentence_citations]\n",
    "\n",
    "        for sc in sentence_citations:\n",
    "            citations.append(sc)\n",
    "\n",
    "    if find_sentence_df[\"usable\"].values[0] == False:\n",
    "        print(\"Invalid: Sentence not usable\")\n",
    "        return None\n",
    "    \n",
    "    merged_citations = merge_citations(citations)\n",
    "    \n",
    "    if not merged_citations:\n",
    "        print(\"Invalid: No citations found\")\n",
    "        return None\n",
    "\n",
    "    citations_without_paragraphs = [c for c in merged_citations if len(c.paragraph_numbers) == 0]\n",
    "    if citations_without_paragraphs:\n",
    "        print(\"Invalid: Found citation without paragraph\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    merged_citations, error = retrieve_citations(merged_citations)\n",
    "    if error:\n",
    "        print(\"Invalid: Error retrieving citations\")\n",
    "        return None\n",
    "    \n",
    "    return merged_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"question\", \"answer\", \"guide\", \"paragraphs\", \"citations\", \"prompt_id\"])\n",
    "\n",
    "for i, row in echr_qa_dataset_df.iterrows():\n",
    "    answer = row['answer']\n",
    "    if answer[0] == \"[\":\n",
    "        sentences = json.loads(answer)\n",
    "    else:\n",
    "        print(\"Already processed\")\n",
    "        df = df._append({\n",
    "            \"question\": row['question'],\n",
    "            \"answer\": row['answer'],\n",
    "            \"guide\": row['guide'],\n",
    "            \"paragraphs\": row['paragraphs'],\n",
    "            \"citations\": json.dumps(json.loads(row['citations']), indent=4),\n",
    "            \"prompt_id\": row['prompt_id']\n",
    "        }, ignore_index=True)\n",
    "        continue\n",
    "    \n",
    "    citations = []\n",
    "\n",
    "    if \"v.\" in row['question'] or \".?\" in row['question']:\n",
    "        print(\"Invalid: Question contains v. or is not a question (.?)\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        merged_citations = get_citations(sentences)\n",
    "        if not merged_citations:\n",
    "            continue\n",
    "\n",
    "        print(\"Found valid QA pair\")\n",
    "        df = df._append({\n",
    "            \"question\": row['question'],\n",
    "            \"answer\": \" \".join(sentences),\n",
    "            \"guide\": row['guide'],\n",
    "            \"paragraphs\": row['paragraphs'],\n",
    "            \"citations\": json.dumps([c.model_dump() for c in merged_citations], indent=4),\n",
    "            \"prompt_id\": row['prompt_id']\n",
    "        }, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Invalid: {e}\")\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/echr_qa_dataset_v4_with_citations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/echr_qa_dataset_v4_with_citations.csv')\n",
    "for i, row in df.iterrows():\n",
    "    json.loads(row['citations'])\n",
    "\n",
    "\n",
    "print(df.at[5, 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_DROP = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows with NaN values\n",
    "df = df.dropna()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_indices = []\n",
    "for i, row in df.iterrows():\n",
    "    citations = json.loads(row[\"citations\"])\n",
    "    for c in citations:\n",
    "        if 1 in c[\"paragraph_numbers\"] or 2 in c[\"paragraph_numbers\"] or 3 in c[\"paragraph_numbers\"] or 4 in c[\"paragraph_numbers\"] or 5 in c[\"paragraph_numbers\"]:\n",
    "            drop_indices.append(i)\n",
    "            break\n",
    "\n",
    "df = df.drop(drop_indices)\n",
    "print(len(df))\n",
    "df.to_csv('data/echr_qa_dataset_v4_with_citations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check for very similar questions\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "questions_with_embeddings_df = df.copy()\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "question_embeddings = embeddings.embed_documents(questions_with_embeddings_df[\"question\"].tolist())\n",
    "\n",
    "OPENAI_EMBEDDINGS = \"openai_embeddings\"\n",
    "questions_with_embeddings_df[OPENAI_EMBEDDINGS] = question_embeddings\n",
    "\n",
    "\n",
    "questions_with_embeddings_df.to_csv('data/echr_qa_dataset_v3_with_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def get_top_n_similarities(\n",
    "    q_embedding, n: int = 5\n",
    "):\n",
    "    df_copy = questions_with_embeddings_df.copy()\n",
    "    df_copy[\"similarity\"] = df_copy[OPENAI_EMBEDDINGS].apply(\n",
    "        lambda x: 1 - cosine(x, q_embedding)\n",
    "    )\n",
    "\n",
    "    return df_copy.nlargest(n, \"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_indices = []\n",
    "do_not_drop = []\n",
    "for i, row in questions_with_embeddings_df.iterrows():\n",
    "    if row[\"question\"] in do_not_drop:\n",
    "        continue\n",
    "    print(\"Q:\", row[\"question\"])\n",
    "    top_similar = get_top_n_similarities(row[OPENAI_EMBEDDINGS], n=2)\n",
    "    top_similar_question = top_similar[\"question\"].values[1]\n",
    "    top_similar_similarity = top_similar[\"similarity\"].values[1]\n",
    "    print(\"MS:\", top_similar_question)\n",
    "    print(\"Sim:\", top_similar_similarity)\n",
    "    do_not_drop.append(top_similar_question)\n",
    "    if top_similar_similarity > 0.85:\n",
    "        drop_indices.append(i)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "print(\"Dropping:\", len(drop_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(drop_indices)\n",
    "print(len(df))\n",
    "df.to_csv('data/echr_qa_dataset_v4_with_citations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases map to df\n",
    "# ! I should collect case name and year for each case\n",
    "as_list = []\n",
    "\n",
    "for case_id, paragraphs in cases_map.items():\n",
    "    if len(paragraphs) < 6:\n",
    "        print(f\"Case id: {case_id} has less than 3 paragraphs\")\n",
    "    for paragraph_number, paragraph_text in paragraphs.items():\n",
    "        as_list.append({'case_id': case_id, 'paragraph_number': paragraph_number, 'paragraph_text': paragraph_text})\n",
    "    \n",
    "cases_map_df = pd.DataFrame(as_list)\n",
    "cases_map_df.to_csv('data/cases.csv', index=False)\n",
    "print(len(cases_map_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('data/echr_qa_dataset_with_citations_final.csv')\n",
    "\n",
    "for i, row in final_df.iterrows():\n",
    "    citations = row['citations']\n",
    "    citations = json.loads(citations)\n",
    "    citations = [Citation(**c) for c in citations]\n",
    "\n",
    "print(len(final_df))\n",
    "\n",
    "questions_with_cases = 0\n",
    "\n",
    "for i, row in final_df.iterrows():\n",
    "    question = row['question']\n",
    "    if \"v.\" in question:\n",
    "        questions_with_cases += 1\n",
    "\n",
    "print(questions_with_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "df = pd.read_csv('echr_qa_dataset_final.csv')\n",
    "print(len(df))\n",
    "for i, row in df.iterrows():\n",
    "    answer = row['answer']\n",
    "    sentences = get_sentences_spacy(answer)\n",
    "    for sentence in sentences:\n",
    "        find_sentence_df = sentences_df[sentences_df['sentence'] == sentence]\n",
    "\n",
    "        if len(find_sentence_df) == 0:\n",
    "            counter += 1\n",
    "            continue\n",
    "\n",
    "print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
