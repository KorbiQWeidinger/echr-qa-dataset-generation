{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/annotated_qa_pairs_with_llm_annotations.csv')\n",
    "\n",
    "order = [\"single-paragraph\", \"multiple-paragraphs\", \"sentence-level-cot\", \"sentence-level-cot-with-search\", \"legal-sentence-level-cot-with-search-v1\", \"legal-sentence-level-cot-with-search-v2\"]\n",
    "\n",
    "df = df.sort_values(by='prompt_id', key=lambda x: x.map({v: i for i, v in enumerate(order)}))\n",
    "\n",
    "prompt_ids = df['prompt_id'].unique()\n",
    "\n",
    "dfs = [df[df['prompt_id'] == prompt] for prompt in prompt_ids]\n",
    "\n",
    "color_scheme = [\"#519DE9\", \"#7CC674\", \"#F6D173\", \"#8481DD\", \"#EF9234\", \"#73C5C5\"]\n",
    "    \n",
    "categories = ['question_relevance', 'question_fluency', 'answer_fluency', 'comprehensiveness', 'conciseness']\n",
    "\n",
    "category_to_label = {\n",
    "    'question_relevance': 'Question Relevance',\n",
    "    'question_fluency': 'Question Fluency',\n",
    "    'answer_fluency': 'Answer Fluency',\n",
    "    'comprehensiveness': 'Answer Comprehensiveness',\n",
    "    'conciseness': 'Answer Conciseness'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 20))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "# plt.suptitle('Comparison of QA Generation Prompts', fontsize=16)\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    ax = axes.flatten()[i]\n",
    "    indices = np.arange(5)\n",
    "    width = 0.15\n",
    "\n",
    "    def to_values(df, category):\n",
    "        d = {1: 0, 2:0, 3:0, 4:0, 5:0}\n",
    "        for k, v in df[category].value_counts().sort_index().to_dict().items():\n",
    "            d[k] = v\n",
    "        return d.values()\n",
    "\n",
    "    for j, prompt_id in enumerate(prompt_ids):\n",
    "        ax.bar(indices + width * j, to_values(dfs[j], category), width=width, label=prompt_id, color=color_scheme[j % len(color_scheme)])\n",
    "    \n",
    "    ax.set_title(category_to_label[category])\n",
    "    ax.set_xlabel('Rating')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_xticks(indices + width)\n",
    "    ax.set_xticklabels(['1', '2', '3', '4', '5'])\n",
    "    ax.legend()\n",
    "\n",
    "# accept all in the df that have a rating of >= 4\n",
    "accepted = df[(df['question_relevance'] >= 4) & (df['question_fluency'] >= 4) & (df['answer_fluency'] >= 4) & (df['comprehensiveness'] >= 4) & (df['conciseness'] >= 4)]\n",
    "# accepted = df[(df['comprehensiveness'] >= 4) & (df['conciseness'] >= 4)]\n",
    "\n",
    "accepted_by_prompt = accepted.groupby('prompt_id').size()\n",
    "total_by_prompt = df.groupby('prompt_id').size()\n",
    "\n",
    "# size of accepted divided by size of all\n",
    "print(\"Percentage accepted total\", accepted.size / df.size)\n",
    "print(\"Accepted by prompt\")\n",
    "print(accepted_by_prompt)\n",
    "print(accepted_by_prompt / total_by_prompt)\n",
    "\n",
    "# sort accepted_by_prompt by order\n",
    "accepted_by_prompt = accepted_by_prompt[order]\n",
    "\n",
    "# transform accepted_by_prompt to a list\n",
    "accepted_by_prompt = accepted_by_prompt.to_dict().values()\n",
    "\n",
    "ax = axes.flatten()[-1]\n",
    "\n",
    "bars = ax.bar(['1', '2', '3', '4', '5', '6'], accepted_by_prompt, color=color_scheme)\n",
    "\n",
    "# Add labels to the bars\n",
    "for bar, prompt_id in zip(bars, order):\n",
    "    bar.set_label(prompt_id)\n",
    "\n",
    "ax.set_title('Total Pairs with all Scores >=4')\n",
    "ax.set_ylabel('Count')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/annotated_qa_pairs.csv')\n",
    "\n",
    "order = [\"single-paragraph\", \"multiple-paragraphs\", \"sentence-level-cot\", \"sentence-level-cot-with-search\", \"legal-sentence-level-cot-with-search-v1\", \"legal-sentence-level-cot-with-search-v2\"]\n",
    "\n",
    "df_single_paragraph = df[df['prompt_id'] == 'single-paragraph']\n",
    "df_multiple_paragraphs = df[df['prompt_id'] == 'multiple-paragraphs']\n",
    "df_sentence_level_cot = df[df['prompt_id'] == 'sentence-level-cot']\n",
    "df_sentence_level_cot_with_search = df[df['prompt_id'] == 'sentence-level-cot-with-search']\n",
    "df_legal_sentence_level_cot_with_search_v1 = df[df['prompt_id'] == 'legal-sentence-level-cot-with-search-v1']\n",
    "df_legal_sentence_level_cot_with_search_v2 = df[df['prompt_id'] == 'legal-sentence-level-cot-with-search-v2']\n",
    "print(\"prompt & Question Relevance & Question Fluency & Answer Fluency & Comprehensiveness & Conciseness \")\n",
    "print(f\"single-paragraph & {df_single_paragraph['question_relevance'].mean()} & {df_single_paragraph['question_fluency'].mean()} & {df_single_paragraph['answer_fluency'].mean()} & {df_single_paragraph['comprehensiveness'].mean()} & {df_single_paragraph['conciseness'].mean()}\")\n",
    "print(f\"multiple-paragraphs & {df_multiple_paragraphs['question_relevance'].mean()} & {df_multiple_paragraphs['question_fluency'].mean()} & {df_multiple_paragraphs['answer_fluency'].mean()} & {df_multiple_paragraphs['comprehensiveness'].mean()} & {df_multiple_paragraphs['conciseness'].mean()}\")\n",
    "print(f\"sentence-level-cot & {df_sentence_level_cot['question_relevance'].mean()} & {df_sentence_level_cot['question_fluency'].mean()} & {df_sentence_level_cot['answer_fluency'].mean()} & {df_sentence_level_cot['comprehensiveness'].mean()} & {df_sentence_level_cot['conciseness'].mean()}\")\n",
    "print(f\"sentence-level-cot-with-search & {df_sentence_level_cot_with_search['question_relevance'].mean()} & {df_sentence_level_cot_with_search['question_fluency'].mean()} & {df_sentence_level_cot_with_search['answer_fluency'].mean()} & {df_sentence_level_cot_with_search['comprehensiveness'].mean()} & {df_sentence_level_cot_with_search['conciseness'].mean()}\")\n",
    "print(f\"legal-sentence-level-cot-with-search-v1 & {df_legal_sentence_level_cot_with_search_v1['question_relevance'].mean()} & {df_legal_sentence_level_cot_with_search_v1['question_fluency'].mean()} & {df_legal_sentence_level_cot_with_search_v1['answer_fluency'].mean()} & {df_legal_sentence_level_cot_with_search_v1['comprehensiveness'].mean()} & {df_legal_sentence_level_cot_with_search_v1['conciseness'].mean()}\")\n",
    "print(f\"legal-sentence-level-cot-with-search-v2 & {df_legal_sentence_level_cot_with_search_v2['question_relevance'].mean()} & {df_legal_sentence_level_cot_with_search_v2['question_fluency'].mean()} & {df_legal_sentence_level_cot_with_search_v2['answer_fluency'].mean()} & {df_legal_sentence_level_cot_with_search_v2['comprehensiveness'].mean()} & {df_legal_sentence_level_cot_with_search_v2['conciseness'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('data/annotated_qa_pairs_with_llm_annotations.csv')\n",
    "\n",
    "# Define the order of categories to sort the DataFrame\n",
    "order = [\"single-paragraph\", \"multiple-paragraphs\", \"sentence-level-cot\", \"sentence-level-cot-with-search\", \"legal-sentence-level-cot-with-search-v1\", \"legal-sentence-level-cot-with-search-v2\"]\n",
    "df = df.sort_values(by='prompt_id', key=lambda x: x.map({v: i for i, v in enumerate(order)}))\n",
    "\n",
    "# Get unique prompt IDs and filter the DataFrame\n",
    "prompt_ids = df['prompt_id'].unique()\n",
    "dfs = [df[df['prompt_id'] == prompt] for prompt in prompt_ids]\n",
    "\n",
    "# Define color scheme and categories\n",
    "color_scheme = [\"#519DE9\", \"#73C5C5\", \"#8481DD\", \"#F6D173\", \"#EF9234\", \"#7CC674\"]\n",
    "\n",
    "categories = ['question_relevance', 'question_fluency', 'answer_fluency', 'comprehensiveness', 'conciseness']\n",
    "\n",
    "# Map category names to more user-friendly labels\n",
    "category_to_label = {\n",
    "    'question_relevance': 'Question Relevance',\n",
    "    'question_fluency': 'Question Fluency',\n",
    "    'answer_fluency': 'Answer Fluency',\n",
    "    'comprehensiveness': 'Answer Comprehensiveness',\n",
    "    'conciseness': 'Answer Conciseness'\n",
    "}\n",
    "\n",
    "# Create a 3x2 grid of subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 15), constrained_layout=True)\n",
    "\n",
    "# Plotting loop\n",
    "for i, category in enumerate(categories):\n",
    "    ax = axes.flatten()[i]\n",
    "    indices = np.arange(5)\n",
    "    width = 0.15\n",
    "\n",
    "    # Helper function to convert DataFrame values for plotting\n",
    "    def to_values(df, category):\n",
    "        d = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}\n",
    "        for k, v in df[category].value_counts().sort_index().to_dict().items():\n",
    "            d[k] = v\n",
    "        return list(d.values())\n",
    "\n",
    "    # Plot bars for each prompt\n",
    "    for j, prompt_id in enumerate(prompt_ids):\n",
    "        ax.bar(indices + width * j, to_values(dfs[j], category), width=width, label=prompt_id, color=color_scheme[j % len(color_scheme)])\n",
    "\n",
    "    # Configure subplot aesthetics\n",
    "    ax.set_title(category_to_label[category], fontsize=12)\n",
    "    ax.set_xlabel('Rating', fontsize=10)\n",
    "    ax.set_ylabel('Count', fontsize=10)\n",
    "    ax.set_xticks(indices + width)\n",
    "    ax.set_xticklabels(['1', '2', '3', '4', '5'])\n",
    "\n",
    "# Place the legend inside the first subplot, increasing font size\n",
    "axes[0, 0].legend(fontsize=12, loc='upper left')\n",
    "\n",
    "# accept all in the df that have a rating of >= 4\n",
    "accepted = df[(df['question_relevance'] >= 4) & (df['question_fluency'] >= 4) & (df['answer_fluency'] >= 4) & (df['comprehensiveness'] >= 4) & (df['conciseness'] >= 4)]\n",
    "# accepted = df[(df['comprehensiveness'] >= 4) & (df['conciseness'] >= 4)]\n",
    "\n",
    "accepted_by_prompt = accepted.groupby('prompt_id').size()\n",
    "total_by_prompt = df.groupby('prompt_id').size()\n",
    "\n",
    "# size of accepted divided by size of all\n",
    "print(\"Percentage accepted total\", accepted.size / df.size)\n",
    "print(\"Accepted by prompt\")\n",
    "print(accepted_by_prompt)\n",
    "print(accepted_by_prompt / total_by_prompt)\n",
    "\n",
    "# sort accepted_by_prompt by order\n",
    "accepted_by_prompt = accepted_by_prompt[order]\n",
    "\n",
    "# transform accepted_by_prompt to a list\n",
    "accepted_by_prompt = accepted_by_prompt.to_dict().values()\n",
    "\n",
    "ax = axes.flatten()[-1]\n",
    "\n",
    "bars = ax.bar(['1', '2', '3', '4', '5', '6'], accepted_by_prompt, color=color_scheme)\n",
    "\n",
    "# Add labels to the bars\n",
    "for bar, prompt_id in zip(bars, order):\n",
    "    bar.set_label(prompt_id)\n",
    "\n",
    "ax.set_title('Total Pairs with all Scores >=4')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
