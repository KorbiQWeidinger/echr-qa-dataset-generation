{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Assurance Measures\n",
    "\n",
    "We use kendall tau as well as TP, FP, TN, FN to assess weather our filters are helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "def calculate_measures(expert_annotations, llm_annotations, threshold_llm=4, threshold_expert=4):\n",
    "    tau, p_value = None, None\n",
    "    try:\n",
    "        tau, p_value = kendalltau(expert_annotations, llm_annotations)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    llm_annotations = [1 if y >= threshold_llm else 0 for y in llm_annotations]\n",
    "    expert_annotations = [1 if y >= threshold_expert else 0 for y in expert_annotations]\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(expert_annotations, llm_annotations).ravel() \n",
    "    f1 = f1_score(expert_annotations, llm_annotations)\n",
    "    \n",
    "    return tp, fp, tn, fn, f1, tau, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_score(response: str, score: str = \"Final\"):\n",
    "    match = re.search(rf\"{score} Score:.*?\\s*.*?(\\d+)\", response, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Score not found in the response: {response}\")\n",
    "    return int(match.group(1))\n",
    "\n",
    "print(get_score(\"Final Score: 3/5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERING_PROMPT = \"\"\"\n",
    "You are a strict legal expert judging ECHR legal question-answer pairs. The answer might be bad, so be strict!\n",
    "\n",
    "Question: {question}\n",
    "Potential Answer: {answer}\n",
    "\n",
    "You MUST answer each question in full sentences!\n",
    "\n",
    "The response MUST follow this template:\n",
    "Comprehensiveness Analysis: {{Go through the answer and analyze how well it answers the question. Does is cover all angles of the question?}}\n",
    "Comprehensiveness Score: {{A score from 1 (not comprehensive at all) to 5 (extremely comprehensive)}}\n",
    "Conciseness: {{Is there any part in the answer irrelevant / unrelated to the question? If so, what is unneeded?}}\n",
    "Conciseness Score: {{A score from 1 (not concise at all) to 5 (extremely concise)}}\n",
    "Answer Fluency: {{Are there any bad sentence transitions in the answer? Are the sentences ordered correctly? Does the answer start with text clearly continuing previous text that is not there?}}\n",
    "Answer Fluency Score: {{A score from 1 (not fluent) to 5 (perfectly fluent)}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/annotated_qa_pairs.csv\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "expert_annotations = []\n",
    "llm_annotations = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    question = row[\"question\"]\n",
    "    answer = row[\"answer\"]\n",
    "\n",
    "    prompt = PromptTemplate.from_template(FILTERING_PROMPT).format(\n",
    "        question=question, answer=answer\n",
    "    )\n",
    "\n",
    "    result = llm.invoke(prompt)\n",
    "\n",
    "    llm_fluency = get_score(result.content, \"Answer Fluency\")\n",
    "    llm_conciseness = get_score(result.content, \"Conciseness\")\n",
    "    llm_comprehensiveness = get_score(result.content, \"Comprehensiveness\")\n",
    "\n",
    "    fluency = row[\"answer_fluency\"]\n",
    "    conciseness = row[\"conciseness\"]\n",
    "    comprehensiveness = row[\"comprehensiveness\"]\n",
    "\n",
    "    expert_annotation = min(fluency, conciseness, comprehensiveness)\n",
    "    llm_annotation = min(llm_fluency, llm_conciseness, llm_comprehensiveness)\n",
    "\n",
    "    print(f\"Expert: {expert_annotation}, LLM: {llm_annotation}\")\n",
    "\n",
    "    df.loc[i, \"expert_annotation_min\"] = expert_annotation\n",
    "    df.loc[i, \"llm_annotation\"] = llm_annotation\n",
    "    df.loc[i, \"llm_answer_fluency\"] = get_score(result.content, \"Answer Fluency\")\n",
    "    df.loc[i, \"llm_conciseness\"] = get_score(result.content, \"Conciseness\")\n",
    "    df.loc[i, \"llm_comprehensiveness\"] = get_score(result.content, \"Comprehensiveness\")\n",
    "    df.loc[i, \"llm_response\"] = result.content\n",
    "    df.to_csv(\"data/annotated_qa_pairs_with_llm_annotations.csv\", index=False)\n",
    "\n",
    "    llm_annotations.append(llm_annotation)\n",
    "    expert_annotations.append(expert_annotation)\n",
    "\n",
    "    try:\n",
    "        tp, fp, tn, fn, f1, tau, p_value = calculate_measures(expert_annotations, llm_annotations)\n",
    "\n",
    "        print(\"EXPERT:\", expert_annotations)\n",
    "        print(\"LLM   :\", llm_annotations)\n",
    "        print(f\"F1 Score: {f1}, True Positives: {tp}, False Positives: {fp}, True Negatives: {tn}, False Negatives: {fn}\")\n",
    "        print(f\"Kendall's Tau: {tau}, p-value: {p_value}\")\n",
    "        print(\"Percentage good before filter:\", (tp + fn) / (tp + fp + tn + fn))\n",
    "        print(\"Percentage good after filter:\", tp / (tp + fp))\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/annotated_qa_pairs_with_llm_annotations.csv\")\n",
    "\n",
    "print(df[\"prompt_id\"].value_counts())\n",
    "\n",
    "# df = df[(df[\"prompt_id\"] != \"single-paragraph\") & (df[\"prompt_id\"] != \"multiple-paragraphs\") & (df[\"prompt_id\"] != \"sentence-level-cot\")]\n",
    "\n",
    "tp, fp, tn, fn, f1, tau, p_value = calculate_measures(df[\"expert_annotation_min\"], df[\"llm_annotation\"], threshold_llm=4, threshold_expert=4)\n",
    "\n",
    "print(\"OVERALL\")\n",
    "print(f\"& {f1} & {tp} & {fp} & {tn} & {fn} & {tau} & {p_value} \\\\\\\\\")\n",
    "print(\"Percentage good before filter:\", (tp + fn) / (tp + fp + tn + fn))\n",
    "print(\"Percentage good after filter:\", tp / (tp + fp))\n",
    "print()\n",
    "\n",
    "tp, fp, tn, fn, f1, tau, p_value = calculate_measures(df[\"conciseness\"], df[\"llm_conciseness\"], threshold_llm=4, threshold_expert=4)\n",
    "\n",
    "print(\"CONCISENESS\")\n",
    "print(f\"& {f1} & {tp} & {fp} & {tn} & {fn} & {tau} & {p_value} \\\\\\\\\")\n",
    "print(\"Percentage good before filter:\", (tp + fn) / (tp + fp + tn + fn))\n",
    "print(\"Percentage good after filter:\", tp / (tp + fp))\n",
    "print()\n",
    "\n",
    "tp, fp, tn, fn, f1, tau, p_value = calculate_measures(df[\"comprehensiveness\"], df[\"llm_comprehensiveness\"], threshold_llm=4, threshold_expert=4)\n",
    "\n",
    "print(\"COMPREHENSIVENESS\")\n",
    "print(f\"& {f1} & {tp} & {fp} & {tn} & {fn} & {tau} & {p_value} \\\\\\\\\")\n",
    "print(\"Percentage good before filter:\", (tp + fn) / (tp + fp + tn + fn))\n",
    "print(\"Percentage good after filter:\", tp / (tp + fp))\n",
    "\n",
    "\n",
    "tp, fp, tn, fn, f1, tau, p_value = calculate_measures(df[\"answer_fluency\"], df[\"llm_answer_fluency\"], threshold_llm=4, threshold_expert=4)\n",
    "\n",
    "print(\"FLUENCY\")\n",
    "print(f\"& {f1} & {tp} & {fp} & {tn} & {fn} & {tau} & {p_value} \\\\\\\\\")\n",
    "print(\"Percentage good before filter:\", (tp + fn) / (tp + fp + tn + fn))\n",
    "print(\"Percentage good after filter:\", tp / (tp + fp))\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
